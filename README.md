# Question answering in legal contracts (RU) ğŸ’¬
Assignment completed as part of the selection process for the DS internship at Kontur
## Stack of technologies ğŸ—
- Python ğŸ
- Transformers ğŸ¤—
- Wandb ğŸ“Š
## Task description ğŸ“‹
The task is to find a piece of text in the contract that corresponds to one of the queries: "Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²" or "Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ĞºÑ‚Ğ°". 

A detailed description of the task can be found in [task_description.md](https://github.com/miglss/QA-document-parts/blob/main/task_description.md)
## Proposed solution ğŸ’¡
For this task mdeberta-v3-base-squad2 model from ğŸ¤— was fine-tuned. Custom tokenizer was also trained on a new corpus.

Hyperparameters optimization was performed using Wandb sweep's: 

<img src="images/hyperparameter_optim.png" alt="map" width="800"/>

After that, model was trained for 30 epochs using best set of hyperparameters

Although a smaller number of epochs could have been avoided, as can be seen from the losses graph:

<img src="images/train_val_loss.png" alt="map" width="800"/>

Quality on validation set during training:

<img src="images/metrics.png" alt="map" width="800"/>

Final quality on test:
- exact score: 84.44444444444444
- f1-score: 97.47689267517949

## Leaderboard
Final position in leaderboard - TOP 5:

<img src="images/leaderboard_position.jpg" alt="map" width="500"/>

## How to improve ğŸ”¨
1. I checked only mdeberta-v3-base-squad2 model, but other models can show a better results 
2. Expanding the sample in some artificial way can improve the final quality of the model 
